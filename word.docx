UNIVERSITY OF CALIFORNIA, RIVERSIDE


			

A Web Crawler for .edu sites.

CS-172 Introduction to Information Retrieval.
				
     

 			             Submitted    by
Ajit Upadya     sid: 861244487
Sandesh Basrur    sid: 861244757
November 3, 2015


                                               Guided by

Mohiuddin Qader




				







1. Collaboration Details:
Ajit Upadya:
Workspace setup
Indexing of files
Page-rank implementation
Servlet implementation

Sandesh Basrur:
JSP pages
CSS
AJAX
Constructing HTML table..

2. System Requirements:
Java 1.7.
To handle Unicode characters, the character encoding has been set to UTF-8.
Maven build tool has been used to integrate and resolve dependencies.
JSoup 1.8.3 java library to parse HTML and extract links. 
Apache commons-validator 1.4.0 to validate and verify the correctness of a URL.
Lucene, a text search engine library implemented in Java. 

3.  Overview of the System
	The web crawler serves the purpose of crawling through an .edu website and finds data from all the hyperlinks found in the pages. This Web Crawler starts with an initial URL called seed. It extracts all the links present in the seed URL and normalizes it, if necessary. It saves those links in a queue called frontier. The Crawler downloads content from these links in the queue based on certain rules. It retrieves only http sites and ignores https sites. It does not download pdfs, videos, css files, common image files (.jpg, .jpeg, .png, .gif) and JavaScript files. It follows crawler ethics by not violating the rules in robots.txt files of the respective sites.























3a. Architecture



HomeController.java
	It takes user input and strips the input text file to fetch multiple seed URLs ( allows only .edu seeds) and starts the crawling process by invoking thread controller. If the input seeds do not meet the criteria of a valid URL, an error message is shown to the user and the application exits.

ThreadController.java 
      The method reads a list of seed URL and starts a new instance of Runnable Thread in its own Thread. It makes use of ExecutorService to create a fixed thread pool to manage Thread allocation.

RunnableThread.java
	This class implements Runnable interface and overrides run() method to start the Thread job. A thread safe queue is used which serves as the frontier. 
	
RobotRule.java
	A  POJO class which overrides the toString() method. It is consumed while checking if a particular URL is allowed in robots.txt.
	
Frontier.java
	This class implements queue and provides custom implementation for all its methods. 


3b. Crawler Strategy
	
add seed URL to queue
while queue is not empty and count < maxNumOfPages
	remove URL from queue
	validate the correctness of URL and check crawling permissions
		download the page to the output directory
		while depth < maxDepth
		extract the links from the page
		add valid links to queue, if they do not already exist	

3c. Data structures used: 
1. Concurrent linked queue<String>: To implement frontier.
2. ExecutorService : To maintain thread pool and allocate the resources judiciously. 
3. Collections.Synchronized List : For thread safe ArrayList.


4 Limitations
A better snippet algorithm similar to Google search-engine could have been implemented.

5 Instructions on deploying the system
Please ensure JAVA_HOME path has been set. Navigate to folder containing web-crawler.sh. Run the web-crawler.sh file passing the following parameters:
{number of pages} {seed file path} {output directory} {number of hops to crawl}

6 Screenshots of the Web Crawler

7 Extra features:
Multithreading: Implemented multithreading with fixed thread pool.
Duplicate URLs and bookmarked URLs have been handled





User interface:
We have used the Model-view-controller(MVC) architectural pattern to implement the user-interface. We have divided the application into following components:
View: responsible for displaying the data to the user.
Model: responsible for maintaining data
Controller: responsible for co-ordinating data flow between view and model.

Model: 
	The IndexService, PagerankService and GraphService classes are used to calculate the index, page rank , parse the user query and fetch the top-scoring documents, sort them in decreasing order of scores and show them to the user. The ResultDoc class which is a POJO (Plain old java object) interacts with the file-system 

A List<ResutlDoc> is populated by finding the top-scoring documents and the same is consumed by the AJAX request. 

An HTML table is constructed in JavaScript to show the results to the user. All the search results are clickable and the pages are opened in a new tab of the browser.



















View:









View: The application consist of a Java Server Pages(JSP) called Home.jsp that accepts input from the user and displays the result to user. Once the user enters the query, the search happens through Asynchronous (AJAX) calls. The data is returned in JSON format and appended to the JSP using Javascript.
ASYNCHRONOUS SEARCH CALLS are made for giving a seemless experience to the user.

The User-interface is impemented using HTML 5 and CSS3 providing a user-friendly theme. Features such as “Show all results” allows users to see all the results retrieved by the query.

Scoring Algorithm: 


	Different scoring types such as “tf.idf”, page-rank, “tf-idf and page_rank” have been implemented to fetch results with varied search techniques. The user can select one of the above scoring types to view results based on the corresponding scoring algorithms.
Boost(index) has been used to boost the title(5) and body(5) during the index time. 
Based on the type of scoring selected, the weights to different terms are calculated as follows :
If tf.idf is selected : 	text^1, title^1.5, meta^1.5
Else if pageRank is selected : 	text^1, title^1, meta^1 is used and pagerank is calculated separately.
Else if tf.idf & PageRank is selected : 	  text^1, title^1.5, meta^1.5 is used and the score is multiplied by the pagerank score of that document.
Lucene’s TopScoreCollector class is used to calculate the top 10 documents for the query. Also, if the user has checked ‘Show all results’, then all relevant documents are fetched and displayed to the user.

Snippet generation algorithm has been implemented.  Which will show a sample of the page content to the user.
The search result are sorted based on the score in decreasing order.





Controller:
	Home controller extends HttpServlet and implements doGet(). It accepts the http request to home page and forwards the request and response objects to home.jsp. 
Search controller is repsonsible for retrieving the results according the query and ranking algorithm and sends it to home.jsp. SearchController code has been merged with phase-1 code a. This controller is responsible for querying the index to find the top scoring documents, convert them into JSON format and send it back to the ajax request.


Page-Rank:


Page rank algorithm has been implemented by using a graph like datastructures with vertices and edges representing the pages and its corresponding incoming and outgoing urls. An epsilon value of 0.1 has been used as the convergence value so the page rank is calculated after a number of iterations. 

While parsing the query, different weights are given to title, body and meta tags to calculate the correct score for the document.

Lucene boost has been used to give more weightage to the title of the page.By assigning higher boost value for certail parts of the document, we are instructing Lucene to assign higher weightage to such documents. A default boost value for the contents in body-tag has been assigned 3 and the boost –value for the contents in Title is assigned 5.



Instructions to run the search engine :

To run the crawler, please place the war file in tomcat/webapps folder and run startup.sh to start tomcat.
The path to the seed URLS, indexing path and output path has to be set in the file 'config.properties' under src/main/resources folder of the project.
To not crawl every time please comment out the property "seedPath" only and the crawler will not run every time. The indexing happens only the first time.

After placing the war file under tomcat/webapps folder and changing the config.properties, please start the server and enter the following url in a browser.
http://localhost:8090/DynamicCrawler/home
